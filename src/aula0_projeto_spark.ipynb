{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KccqfD8ujL3K"
   },
   "source": [
    "# Começando o Trabalho\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyAh7l9hGlHq"
   },
   "source": [
    "## Apache Spark - Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_pds0odi1Rj"
   },
   "source": [
    "## Utilizando o Spark no Google Colab\n",
    "\n",
    "Para facilitar o desenvolvimento de nosso projeto neste curso vamos utilizar o Google Colab como ferramenta e para configurar o PySpark basta executar os comandos abaixo na própria célula do seu *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JmcO4uWZ3OLO"
   },
   "source": [
    "# instalar as dependências\n",
    "!apt-get update -qq\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_4Z_1m0z3sPJ"
   },
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kqdft9fpGlH0"
   },
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIZNca7Pjgqf"
   },
   "source": [
    "# Carregamento de Dados\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGNS075GGlH0"
   },
   "source": [
    "## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
    "\n",
    "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
    "\n",
    "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrão de construtor:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yayRGdn_GlH1"
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Iniciando com Spark\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PizPNqbmCuSM"
   },
   "source": [
    "spark"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbOcx1oXGlH4"
   },
   "source": [
    "## DataFrames com Spark\n",
    "\n",
    "\n",
    "### Interfaces Spark\n",
    "\n",
    "Existem três interfaces principais do Apache Spark que você deve conhecer: Resilient Distributed Dataset, DataFrame e Dataset.\n",
    "\n",
    "- **Resilient Distributed Dataset**: A primeira abstração do Apache Spark foi o Resilient Distributed Dataset (RDD). É uma interface para uma sequência de objetos de dados que consiste em um ou mais tipos localizados em uma coleção de máquinas (um cluster). Os RDDs podem ser criados de várias maneiras e são a API de “nível mais baixo” disponível. Embora esta seja a estrutura de dados original do Apache Spark, você deve se concentrar na API DataFrame, que é um superconjunto da funcionalidade RDD. A API RDD está disponível nas linguagens Java, Python e Scala.\n",
    "\n",
    "- **DataFrame**: Trata-se de um conceito similar ao DataFrame que você pode estar familiarizado como o pacote pandas do Python e a linguagem R . A API DataFrame está disponível nas linguagens Java, Python, R e Scala.\n",
    "\n",
    "- **Dataset**: uma combinação de DataFrame e RDD. Ele fornece a interface digitada que está disponível em RDDs enquanto fornece a conveniência do DataFrame. A API Dataset está disponível nas linguagens Java e Scala.\n",
    "\n",
    "Em muitos cenários, especialmente com as otimizações de desempenho incorporadas em DataFrames e Datasets, não será necessário trabalhar com RDDs. Mas é importante entender a abstração RDD porque:\n",
    "\n",
    "- O RDD é a infraestrutura subjacente que permite que o Spark seja executado com tanta rapidez e forneça a linhagem de dados.\n",
    "\n",
    "- Se você estiver mergulhando em componentes mais avançados do Spark, pode ser necessário usar RDDs.\n",
    "\n",
    "- As visualizações na Spark UI fazem referência a RDDs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZpFfzZN0GlH5"
   },
   "source": [
    "data = [('Zeca','35'), ('Eva', '29')]\n",
    "colNames = ['Nome', 'Idade']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3APKGkAlGlH5"
   },
   "source": "df = spark.createDataFrame(data, colNames)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NyBW_ff-JMYs"
   },
   "source": "df.describe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.printSchema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe('Idade').show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.toPandas()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXVfnTIHGlH3"
   },
   "source": [
    "## Projeto\n",
    "\n",
    "Nosso projeto consiste em ler, manipular, tratar e salvar um conjunto de dados volumosos utilizando como ferramenta o Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCYYVAtjhMa7"
   },
   "source": [
    "## Carregamento de dados\n",
    "\n",
    "### Dados Públicos CNPJ\n",
    "#### Receita Federal\n",
    "\n",
    "> [Empresas](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)\n",
    "> \n",
    "> [Estabelecimentos](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)\n",
    "> \n",
    "> [Sócios](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)\n",
    "\n",
    "[Fonte original dos dados](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj)\n",
    "\n",
    "---\n",
    "[property SparkSession.read](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html)\n",
    "\n",
    "[DataFrameReader.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6znrrAdApQmE"
   },
   "source": [
    "### Montando nosso drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6g2I8wYShfYJ"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f6bwFADGlH6"
   },
   "source": [
    "### Carregando os dados das empresas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sffAOcMQt_aR"
   },
   "source": [
    "import zipfile\n",
    "zipfile.ZipFile('/Users/fabioleite/PycharmProjects/curso_spark_uepb/dados_projetos/empresas_cnpj/empresas.zip', 'r').extractall('/Users/fabioleite/PycharmProjects/curso_spark_uepb/dados_projetos/empresas_cnpj/empresas_br')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4RVsaljf8mus"
   },
   "source": [
    "path = '/Users/fabioleite/PycharmProjects/curso_spark_uepb/dados_projetos/empresas_cnpj/empresas_br/empresas'\n",
    "empresas_df = spark.read.csv(path, sep=';', header=False, inferSchema=True)\n",
    "empresas_df.count()\n",
    "empresas_df.printSchema()\n",
    "empresas_df.select()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lendo arquivos particionados"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "empresasColNames = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for index, colName in enumerate(empresasColNames):\n",
    "    empresas_df = empresas_df.withColumnRenamed(f\"_c{index}\", colName)\n",
    "\n",
    "empresas_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Análise dos dados baixados faça análise usando os comandos estudados para dataframe\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "empresas_df.toPandas()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0QOCzKVGlH7"
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnibIqSUGlH_"
   },
   "source": [
    "## Analisando os dados\n",
    "\n",
    "[Data Types](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#data-types)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7BCEdVduXqLP"
   },
   "source": [
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aocivOoxGlIA"
   },
   "source": "empresas_df.printSchema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CNQJP7E6UsGC"
   },
   "source": "empresas_df.limit(5).toPandas()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xUXixnpzZPQe"
   },
   "source": [
    "empresas_df = empresas_df.withColumn('capital_social_da_empresa', f.regexp_replace('capital_social_da_empresa', ',', '.'))\n",
    "empresas_df.limit(5).toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "43jdqE5gGlIA"
   },
   "source": "empresas_df.printSchema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kVXz5ZFqY1sW"
   },
   "source": [
    "empresas_df = empresas_df.withColumn('capital_social_da_empresa', empresas_df['capital_social_da_empresa'].cast(DoubleType()))\n",
    "empresas_df.limit(5).toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "empresas_df.printSchema()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A75QicR-WYxn"
   },
   "source": [
    "## Modificando os tipos de dados\n",
    "\n",
    "[Functions](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#functions)\n",
    "\n",
    "[withColumn](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVElbxNlfFjk"
   },
   "source": [
    "### Convertendo String ➔ Double\n",
    "\n",
    "#### `StringType ➔ DoubleType`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XSDekv4rbodk"
   },
   "source": [
    "df = spark.createDataFrame([(20200924,), (20201022,), (20210215,)], ['data'])\n",
    "df.toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tjbORHHw4M5j"
   },
   "source": "df.printSchema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EyIP_6ge4ZF4"
   },
   "source": [
    "df = df.withColumn(\"data\", f.to_date(df.data.cast(StringType()), 'yyyyMMdd'))\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZUdfkV6EeCJt"
   },
   "source": "df.toPandas()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Carregar e configurar Estabeleciimentos"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vsNOpcZoe20V"
   },
   "source": "estabelecimentos.printSchema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5qUYxtZSGlIE"
   },
   "source": [
    "estabelecimentos = estabelecimentos\\\n",
    "    .withColumn(\n",
    "        \"data_situacao_cadastral\", \n",
    "        f.to_date(estabelecimentos.data_situacao_cadastral.cast(StringType()), 'yyyyMMdd')\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        \"data_de_inicio_atividade\", \n",
    "        f.to_date(estabelecimentos.data_de_inicio_atividade.cast(StringType()), 'yyyyMMdd')\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        \"data_da_situacao_especial\", \n",
    "        f.to_date(estabelecimentos.data_da_situacao_especial.cast(StringType()), 'yyyyMMdd')\n",
    "    )\n",
    "\n",
    "estabelecimentos.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FVxWhP_DZkDC"
   },
   "source": "estabelecimentos.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp_Zv8tAgcbN"
   },
   "source": [
    "### Convertendo String ➔ Date\n",
    "\n",
    "#### `StringType ➔ DateType`\n",
    "\n",
    "[Datetime Patterns](https://spark.apache.org/docs/3.1.2/sql-ref-datetime-pattern.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iRdCvl26o1eC"
   },
   "source": [
    "df = spark.createDataFrame([(20200924,), (20201022,), (20210215,)], ['data'])\n",
    "df.toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_OMaiBT16YAX"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z4alYEILpRZe"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MxB1k7IGp2vo"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0yBmkN9H2-QP"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CgO66CVEUGns"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2L2904mRn5dy"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "39H5_TRgV0j5"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NE_Hf6G3gak_"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mTsUeiapO3Pz"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4rDRJjed8BzS"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zv529a0SGgMF"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg0lldBjGlIB"
   },
   "source": [
    "# Seleções e consultas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3q8yA8kgH9G"
   },
   "source": [
    "## Selecionando informações\n",
    " \n",
    "[DataFrame.select(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.select.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QdMi5XrjbNxA"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X9cgyry3GlIB"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7ilkFXsIbw9j"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j94bdu9r0ykx"
   },
   "source": [
    "## Faça como eu fiz"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "smUC0SgN0sGc"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS3mrMkHZjqX"
   },
   "source": [
    "## Identificando valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8orupk7E9sfp"
   },
   "source": [
    "df = spark.createDataFrame([(1,), (2,), (3,), (None,)], ['data'])\n",
    "df.toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4l4asr_S95MN"
   },
   "source": [
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UYjUKTUa_KzT"
   },
   "source": [
    "df = spark.createDataFrame([(1.,), (2.,), (3.,), (float('nan'),)], ['data'])\n",
    "df.toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lqzQebm7_QAO"
   },
   "source": [
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j8v6WOpb96qU"
   },
   "source": [
    "df = spark.createDataFrame([('1',), ('2',), ('3',), (None,)], ['data'])\n",
    "df.toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nokz_PPC994j"
   },
   "source": [
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N1wvIzb8-Rpk"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i95DWXnV-Usy"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7f2vmhnQZrod"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nlccfXKMavJv"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p_BjVZKeu_V2"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VYp2I6NBaz2C"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8DrHfT6vbSeU"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_NMPijDgp97"
   },
   "source": [
    "## Ordenando os dados\n",
    "\n",
    "[DataFrame.orderBy(*cols, **kwargs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RiEEP4eTd9K_"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PZrd_p7mfFDR"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeMk86UGhZvc"
   },
   "source": [
    "## Filtrando os dados\n",
    "\n",
    "[DataFrame.where(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.where.html) ou [DataFrame.filter(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.filter.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y6ZHVYBHjPa5"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z6tE7LiKXT1I"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ_FrfbOK-1a"
   },
   "source": [
    "## O comando LIKE\n",
    "\n",
    "[Column.like(other)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.like.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R3sMWIEIQLY7"
   },
   "source": [
    "df = spark.createDataFrame([('RESTAURANTE DO RUI',), ('Juca restaurantes ltda',), ('Joca Restaurante',)], ['data'])\n",
    "df.toPandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zfc1p_pEQ1UJ"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eHqcmjIxGlIF"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiyZCFaEQPtQ"
   },
   "source": [
    "# Agregações e Junções\n",
    "---\n",
    "\n",
    "[DataFrame.groupBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)\n",
    "\n",
    "[DataFrame.agg(*exprs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.agg.html)\n",
    "\n",
    "[DataFrame.summary(*statistics)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html)\n",
    "\n",
    "> Funções:\n",
    "[approx_count_distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html) | \n",
    "[avg](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.avg.html) | \n",
    "[collect_list](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_list.html) | \n",
    "[collect_set](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_set.html) | \n",
    "[countDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.countDistinct.html) | \n",
    "[count](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.count.html) | \n",
    "[grouping](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.grouping.html) | \n",
    "[first](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.first.html) | \n",
    "[last](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.last.html) | \n",
    "[kurtosis](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.kurtosis.html) | \n",
    "[max](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.max.html) | \n",
    "[min](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.min.html) | \n",
    "[mean](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.mean.html) | \n",
    "[skewness](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.skewness.html) | \n",
    "[stddev ou stddev_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev.html) | \n",
    "[stddev_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev_pop.html) | \n",
    "[sum](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sum.html) | \n",
    "[sumDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sumDistinct.html) | \n",
    "[variance ou var_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.variance.html) | \n",
    "[var_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.var_pop.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc6YDFOkywQc"
   },
   "source": [
    "## Sumarizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o8p8BS7QQKfS"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xlAbbR4SdZHw"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_oOOQPUqNiRZ"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOCWSMepX5jN"
   },
   "source": [
    "## Juntando DataFrames - Joins\n",
    "\n",
    "[DataFrame.join(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6uklRCkDX4Tf"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eCMHBCrokoc3"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oDaN4XFQkdsE"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VyvClbqRHviC"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nq_pGKucIjri"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wWoVxvYKJkC-"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ta9UsinQJn1w"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kki6ZPfNJ_sI"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EIhL8EKGN09L"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mrIpShtRN3WK"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LZ9OYX7NN8SM"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FdFVRNJeQRua"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RSEJMOF5PxGF"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qARQXzC71hPq"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "26U30E70nTew"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq-9n_n2GlIF"
   },
   "source": [
    "## SparkSQL\n",
    "\n",
    "[SparkSession.sql(sqlQuery)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.sql.html)\n",
    "\n",
    "Para saber mais sobre performance: [Artigo - Spark RDDs vs DataFrames vs SparkSQL](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NGCFVpPnGlIF"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5c_OrvMLGlIF"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5cff25Y3GlIG"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rj0ADzBfGlIG"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RRxV3Crg1ZFo"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q0zjZOxM0jzy"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z0oUrvo4E76z"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ghAEAvfj0eS3"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDM3l-CUGlIJ"
   },
   "source": [
    "# Formas de Armazenamento\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySrkXajNGlIK"
   },
   "source": [
    "## Arquivos CSV\n",
    "\n",
    "[property DataFrame.write](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.write.html)\n",
    "\n",
    "[DataFrameWriter.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f1PBgc6kGlIK"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "42_1Y8eW6e2r"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eD01X0MN6pgp"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4KYGLOUe9VN"
   },
   "source": [
    "## Faça como eu fiz"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8ttmS8v1GlIK"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3YvAdU_MGlIK"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge7vmWujGlIK"
   },
   "source": [
    "## Arquivos PARQUET\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/)\n",
    "\n",
    "[DataFrameWriter.parquet(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qd14cXcto9za"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZysCHmzicRo4"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9siUUauWdAej"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYsfCfudGlIK"
   },
   "source": [
    "## Particionamento dos dados\n",
    "\n",
    "[DataFrameWriter.partitionBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p3lGYb4qRVz3"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i6Tv9DelGlIL"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zFAQ-XlHScMr"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dveTyKd1VitH"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
